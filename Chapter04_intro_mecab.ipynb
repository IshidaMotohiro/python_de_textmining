{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MeCab による形態素解析と抽出語の選択\n",
    "本章では日本語の文章・テキストを統計的に分析するための基礎となる形態素解析器として MeCab を利用する方法を解説します。\n",
    "\n",
    "MeCab を本書指定の方法でインストールしてください。macOS（M1チップ版）、macsOS（Intel版）、Windowsでそれぞれインストール方法が異なります。\n",
    "Windows環境を利用の方は、64bit UTF-8版のMeCabが必要です。もし公式の 32bit版 MeCab をすでにインストールしている場合は、ねんのため、これをアンインストールしてから、64bit版をインストールし直してください。\n",
    "\n",
    "さらに、MeCab と Python を連携させるライブラリが必要です。\n",
    "なお、本書付属の requirements.txt を使い `!pip3 install -r requirements.txt` を実行すると、mecab-python3 だけエラーとなる場合があるようです。\n",
    "その場合は、Jupyter のコマンドセルで `!pip3 install mecab-python3` を実行してください。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install mecab-python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mecab-python3 ライブラリの使い方\n",
    "\n",
    "それでは、実際に形態素解析を実行してみましょう。以下で、簡単な例を示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コロナ 禍 で 二 度目 の 正月 を 迎える 。 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "tagger = MeCab.Tagger('-O wakati')\n",
    "x = 'コロナ禍で二度目の正月を迎える。'\n",
    "x_wakati = tagger.parse(x)\n",
    "print(x_wakati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コロナ禍 で 二 度目 の 正月 を 迎える 。 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 以下は筆者の環境で NEologd 辞書を指定する方法です\n",
    "## -d より右のフォルダ指定を読者の環境にあわせて変更する必要があります\n",
    "tagger = MeCab.Tagger('-O wakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "x = 'コロナ禍で二度目の正月を迎える。'\n",
    "x_wakati = tagger.parse(x)\n",
    "print(x_wakati)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MeCab の辞書のデフォルトを NEologd に変更することもできます。\n",
    "\n",
    "```\n",
    ";\n",
    "; Configuration file of MeCab\n",
    ";\n",
    "; $Id: mecabrc.in,v 1.3 2006/05/29 15:36:08 taku-ku Exp $;\n",
    ";\n",
    "; dicdir =  /usr/local/lib/mecab/dic/ipadic\n",
    "dicdir =  /usr/local/lib/mecab/dic/mecab-ipadic-neologd\n",
    "; userdic = /home/foo/bar/user.dic\n",
    "\n",
    "; output-format-type = wakati\n",
    "; input-buffer-size = 8192\n",
    "\n",
    "; node-format = %m\\n\n",
    "; bos-format = %S\\n\n",
    "; eos-format = EOS\\n\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 品詞情報\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BOS/EOS,*,*,*,*,*,*,*,*\n",
      "お\n",
      "接頭詞,名詞接続,*,*,*,*,お,オ,オ\n",
      "寿司\n",
      "名詞,一般,*,*,*,*,寿司,スシ,スシ\n",
      "を\n",
      "助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "食べ\n",
      "動詞,自立,*,*,一段,連用形,食べる,タベ,タベ\n",
      "た\n",
      "助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n",
      "。\n",
      "記号,句点,*,*,*,*,。,。,。\n",
      "\n",
      "BOS/EOS,*,*,*,*,*,*,*,*\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "## NEologd 辞書を利用する\n",
    "## 以下は筆者の環境で辞書を指定する方法です\n",
    "neologd = '-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd'\n",
    "tagger = MeCab.Tagger(neologd)\n",
    "node = tagger.parseToNode('お寿司を食べた。')\n",
    "while node:\n",
    "    print (node.surface)\n",
    "    print (node.feature) \n",
    "    node = node.next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "分割結果をここでは `node` というオブジェクトに保存しています。\n",
    "\n",
    "実際の分析においては `node.feature` のうち、特定の要素だけを取り出すことになるでしょう。本書では、「品詞」「品詞細分類1」「基本形」を主に利用します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS/EOS * *\n",
      "接頭詞 名詞接続 お\n",
      "名詞 一般 寿司\n",
      "助詞 格助詞 を\n",
      "動詞 自立 食べる\n",
      "助動詞 * た\n",
      "記号 句点 。\n",
      "BOS/EOS * *\n"
     ]
    }
   ],
   "source": [
    "node = tagger.parseToNode('お寿司を食べた。')\n",
    "while node:\n",
    "    elem = node.feature.split(',')\n",
    "    print(elem[0], elem[1], elem[6])\n",
    "    node = node.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "お 接頭詞 名詞接続 お\n",
      "寿司 名詞 一般 寿司\n",
      "を 助詞 格助詞 を\n",
      "食べ 動詞 自立 食べる\n",
      "た 助動詞 * た\n",
      "。 記号 句点 。\n"
     ]
    }
   ],
   "source": [
    "text = 'お寿司を食べた。'\n",
    "node = tagger.parseToNode(text)\n",
    "while node:\n",
    "    if node.surface != '':\n",
    "        elem = node.feature.split(',')\n",
    "        print(node.surface, elem[0], elem[1], elem[6]) \n",
    "    node = node.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipadic\n",
      "今日\t名詞,副詞可能,*,*,*,*,今日,キョウ,キョー\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "午後\t名詞,副詞可能,*,*,*,*,午後,ゴゴ,ゴゴ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "生\t名詞,形容動詞語幹,*,*,*,*,生,ナマ,ナマ\n",
      "協\t名詞,接尾,一般,*,*,*,協,キョウ,キョー\n",
      "で\t助詞,格助詞,一般,*,*,*,で,デ,デ\n",
      "ランチ\t名詞,一般,*,*,*,*,ランチ,ランチ,ランチ\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "食べ\t動詞,自立,*,*,一段,連用形,食べる,タベ,タベ\n",
      "まし\t助動詞,*,*,*,特殊・マス,連用形,ます,マシ,マシ\n",
      "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n",
      "EOS\n",
      "\n",
      "NEologd\n",
      "今日\t名詞,副詞可能,*,*,*,*,今日,キョウ,キョー\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "午後\t名詞,副詞可能,*,*,*,*,午後,ゴゴ,ゴゴ\n",
      "は生\t名詞,サ変接続,*,*,*,*,派生,ハセイ,ハセイ\n",
      "協\t名詞,接尾,一般,*,*,*,協,キョウ,キョー\n",
      "で\t助詞,格助詞,一般,*,*,*,で,デ,デ\n",
      "ランチ\t名詞,一般,*,*,*,*,ランチ,ランチ,ランチ\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "食べ\t動詞,自立,*,*,一段,連用形,食べる,タベ,タベ\n",
      "まし\t助動詞,*,*,*,特殊・マス,連用形,ます,マシ,マシ\n",
      "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "print('ipadic')\n",
    "## 以下は筆者の環境で辞書を指定する方法です\n",
    "tagger = MeCab.Tagger('-d /usr/local/lib/mecab/dic/ipadic/')\n",
    "x = '今日の午後は生協でランチを食べました'\n",
    "x_wakati = tagger.parse(x)\n",
    "print(x_wakati)\n",
    "##\n",
    "print('NEologd')\n",
    "tagger = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd/')\n",
    "x = '今日の午後は生協でランチを食べました'\n",
    "x_wakati = tagger.parse(x)\n",
    "print(x_wakati)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ユーザー辞書\n",
    "\n",
    "繰り返しになりますが MeCab の辞書は完璧ではありません。筆者の名である「基広」は登録されていないため、通常の辞書では「基」と「広」に分割されてしまいます（ちなみに、NEologdだと「石田基広」という固有名詞1語とみなされます）。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "石田\t名詞,固有名詞,人名,姓,*,*,石田,イシダ,イシダ\n",
      "基\t名詞,固有名詞,人名,名,*,*,基,ハジメ,ハジメ\n",
      "広\t形容詞,自立,*,*,形容詞・アウオ段,ガル接続,広い,ヒロ,ヒロ\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "tagger = MeCab.Tagger('-d /usr/local/lib/mecab/dic/ipadic/')\n",
    "x = '石田基広'\n",
    "x_wakati = tagger.parse(x)\n",
    "print(x_wakati)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このような語であれば、ユーザーが独自に辞書を作成して、MeCabに指定することになります。\n",
    "以下は筆者の環境(Ubuntu)で実行する例です。Windowsユーザーは本書の記載を参照ください。\n",
    "\n",
    "[^mecab_dic]: <https://taku910.github.io/mecab/dic.html>\n",
    "\n",
    "\n",
    "> /usr/local/libexec/mecab/mecab-dict-index -d /usr/local/lib/mecab/dic/ipadic/ -u ~./ishida.dic -f utf-8 -t utf-8 ~./ishida.csv\n",
    "\n",
    "これにより user.dic というファイルが生成されるので、MeCab の `Tagger` インスタンスを生成する際に指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "石田\t名詞,固有名詞,人名,姓,*,*,石田,イシダ,イシダ\n",
      "基広\t名詞,固有名詞,人名,名,*,*,基広,モトヒロ,モトヒロ\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 以下は筆者の環境(Ubuntu22.04)で実行する例です\n",
    "path = '-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd -u /home/ishida/ishida.dic'\n",
    "tagger = MeCab.Tagger(path)\n",
    "x = '石田基広'\n",
    "x_wakati = tagger.parse(x)\n",
    "print(x_wakati)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 自作関数のモジュール化\n",
    "\n",
    "さて、形態素解析の結果を使って分析を行う場合、利用する品詞情報は多くの場合ほぼ同じになるでしょう。具体的には、名詞や動詞、形容詞のみを抽出してデータとしたいことがあります。先にみたように、形態素ごとの品詞情報は `node.feature` というリストの最初（つまり0番目）の要素です。そこで、最初の要素が名詞か動詞、あるいは形容詞だった場合だけ形態素をデータとして取り出せば良いわけです。そこで、3品詞だけを取り出す処理を関数として定義してしまいましょう。また、関数定義を別ファイルとして保存して、これをモジュールとして読み込む方法を紹介します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "## my_mecab.py\n",
    "\n",
    "import MeCab\n",
    "## 以下は筆者の環境での辞書指定です。読者の環境に合わせて変更してください。\n",
    "path = '-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd -u /home/ishida/ishida.dic'\n",
    "## 辞書の場所がわからない場合\n",
    "## path =\"\"\n",
    "\n",
    "tagger = MeCab.Tagger(path)\n",
    "\n",
    "def tokens(text, pos = ['名詞','形容詞','動詞']):\n",
    "    node = tagger.parseToNode(text)\n",
    "    word_list = []\n",
    "    while node:\n",
    "        if node.surface != '':\n",
    "            elem = node.feature.split(',')\n",
    "            term = elem[6] if elem[6] != '*' else node.surface\n",
    "            if len(pos) < 1 or elem[0] in pos:\n",
    "                word_list.append(term)\n",
    "        node = node.next\n",
    "    return word_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    out = tokens('ランチを食べました。')\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "このスクリプトを my_mecab.py という名前で保存し、Jupyter を起動しているフォルダに保存しておけば、`import my_mecab` とすることで形態素解析を実行する準備ができることになります。なお、 `if __name__ == '__main__':` 以下の命令はこのスクリプトをターミナル（コマンドプロンプト）などで単独に実行するための命令です。ターミナルで `python my_mecab.py` と入力して Enter を押すと、「ランチを食べました」という文章を形態素解析した結果を表示します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/2bddf92b-47f9-4809-95a5-b91e7f25af27/myData/GitHub/python_de_textmining\n"
     ]
    }
   ],
   "source": [
    "## Jupyterの参照するデフォルトのフォルダを確認\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_janome_stopwords.py\n",
      "my_ginza.py\n",
      "my_tokenizer.py\n",
      "my_janome.py\n",
      "my_mecab.py\n",
      "my_tokenizer_with_stopwords.py\n",
      "AozoraDL.py\n",
      "my_mecab_stopwords.py\n"
     ]
    }
   ],
   "source": [
    "## ここに my_mecab.py が保存されているか確認\n",
    "import glob\n",
    "files = glob.glob('*.py')\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ランチ', '食べる']\n"
     ]
    }
   ],
   "source": [
    "import my_mecab\n",
    "out = my_mecab.tokens('ランチを食べました。')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ストップワード\n",
    "\n",
    "\n",
    "いま「今日は本を読んで過ごした。」を形態素解析してみると、以下のような結果になります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日 名詞\n",
      "は 助詞\n",
      "この 連体詞\n",
      "本 名詞\n",
      "を 助詞\n",
      "読ん 動詞\n",
      "で 助詞\n",
      "過ごし 動詞\n",
      "た 助動詞\n",
      "。 記号\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "text = '今日はこの本を読んで過ごした。'\n",
    "tagger = MeCab.Tagger()\n",
    "node = tagger.parseToNode(text)\n",
    "while node:\n",
    "    if node.surface != '':\n",
    "        elem = node.feature.split(',')\n",
    "        print(node.surface, elem[0]) \n",
    "    node = node.next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "テキストの内容を分析するため、形態素解析の結果から機能語を削除したい場合があります。このためには、分割された形態素ごとにその品詞情報を参照して、必要でない形態素はスキップします。\n",
    "下のコードで `not in` の部分が該当する場合はスキップすることを表しています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "これ 名詞 代名詞 これ\n",
      "良い 形容詞 自立 良い\n",
      "本 名詞 一般 本\n",
      "だ 助動詞 * だ\n",
      "もう一度 副詞 一般 もう一度\n",
      "あと 名詞 一般 あと\n",
      "読み直そ 動詞 自立 読み直す\n",
      "う 助動詞 * う\n"
     ]
    }
   ],
   "source": [
    "text = 'これは良い本だから、もう一度、あとで読み直そう。'\n",
    "func_words = ['助詞', '記号']\n",
    "\n",
    "node = tagger.parseToNode(text)\n",
    "while node:\n",
    "    if node.surface != '':\n",
    "        elem = node.feature.split(',')\n",
    "        if elem[0] not in func_words:\n",
    "            print(node.surface, elem[0], elem[1], elem[6]) \n",
    "    node = node.next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`if elem[0] not in func_words:` という条件文で、品詞情報(`elem[0]`)が、最初の方で定義した `func_words` というリストに含まれていない場合だけ、出力を表示するという処理を行っています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "良い 形容詞 自立 良い\n",
      "本 名詞 一般 本\n",
      "もう 副詞 一般 もう\n",
      "冊 名詞 接尾 冊\n",
      "買っ 動詞 自立 買う\n",
      "永久 名詞 一般 永久\n",
      "保存 名詞 サ変接続 保存\n",
      "版 名詞 接尾 版\n",
      "しよ 動詞 自立 する\n"
     ]
    }
   ],
   "source": [
    "text = 'これは良い本だから、もう一冊買って、永久保存版にしよう。'\n",
    "func_words = ['助詞','記号']\n",
    "func_subwords = ['代名詞', '数', '*']\n",
    "node = tagger.parseToNode(text)\n",
    "while node:\n",
    "    if node.surface != '':\n",
    "        elem = node.feature.split(',')\n",
    "        if elem[0] not in func_words:\n",
    "            if elem[1] not in func_subwords:\n",
    "                print(node.surface, elem[0], elem[1], elem[6]) \n",
    "    node = node.next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "MeCab の品詞情報を利用せず、機能語と判断される形態素をあらかじめリストにしておいて、これと照合することで不要語を一気に削除してしまう方法もあります。こうしたリストを **ストップワード** と言います。分析目的にあわせてストップワードは自身を作成するのがベストですが、公開されているリストを利用することもできます。こうしたリストとして、京都大学情報学研究科社会情報学専攻田中克己研究室が公開している SlothLib [^SlothLib]  がありますので、ここで利用させてもらいましょう。\n",
    "\n",
    "[^SlothLib]: http://www.dl.kuis.kyoto-u.ac.jp/slothlib/\n",
    "\n",
    "ここでは stopword リストを stopwords.txt として、data フォルダに保存します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "urllib.request.urlretrieve(url, 'data/stopwords.txt')\n",
    "stopwords = []\n",
    "with open('data/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "\tstopwords = [w.strip() for w in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "while node:\n",
    "    if node.surface != '':\n",
    "        elem = node.feature.split(',')\n",
    "        if elem[6] not in stopwords:\n",
    "            print(node.surface, elem[0], elem[1], elem[6]) \n",
    "    node = node.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "## my_mecab_stopwords.py\n",
    "\n",
    "import MeCab\n",
    "## 以下は筆者の環境で辞書を指定する方法です\n",
    "path = '-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd'\n",
    "tagger = MeCab.Tagger(path)\n",
    "# tagger = MeCab.Tagger()\n",
    "\n",
    "def tokens(text, pos=['名詞', '形容詞', '動詞'], stopwords_list=[]):\n",
    "    text = ''.join(text.split())\n",
    "    node = tagger.parseToNode(text)\n",
    "    word_list = []\n",
    "    while node:\n",
    "        if node.surface != '':\n",
    "            elem = node.feature.split(',')\n",
    "            term = elem[6] if elem[6] != '*' else node.surface\n",
    "            if term not in stopwords_list:\n",
    "                if len(pos) < 1 or elem[0] in pos:\n",
    "                    word_list.append(term)\n",
    "        node = node.next\n",
    "    return word_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    out = tokens('今日の午後は八宝菜を食べました。')\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['良い', '本']\n"
     ]
    }
   ],
   "source": [
    "import my_mecab_stopwords as mcb\n",
    "out = mcb.tokens('これは良い本です。', stopwords_list=stopwords)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析の対象とする形態素の選択にあたっては、さらに出現回数（頻度）が極端に多い、あるいは少ない形態素を削除することも考えられます。これらについては、以降の実践例の紹介で取り上げます。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "name": "Chapter04_intro_mecab.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
