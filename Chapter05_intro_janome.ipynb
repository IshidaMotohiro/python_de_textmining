{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Janome\n",
    "**Janome** は Python 言語で作成された形態素解析器です。MeCab とは異なり、別のソフトウェアのインストールを前提としません。ただし、解析対象の文章（テキスト）のサイズが大きくなると、解析に時間がかかることが欠点となります。大規模なテキスト群を対象とする場合は、MeCab を使うほうがストレスがありません。\n",
    "\n",
    "\n",
    "Anaconda Prompt を起動します。Amaconda のインストール時にオプションとして Just Me ではなくシステム全体へのインストールを行った場合は、右クリック「その他->管理者として起動」として起動します。以下のように打ち込み、janomeをインストールします。\n",
    "\n",
    "> conda install -c conda-forge janome \n",
    "\n",
    "上のコマンドが機能しない場合は、下のようにします。\n",
    " \n",
    "> pip install janome\n",
    "\n",
    "Jupyterのセルからも実行できます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting janome\n",
      "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: janome\n",
      "Successfully installed janome-0.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install janome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "インストールしたら、Janome を試してみましょう。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "toks = t.tokenize('すもももももももものうち')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "すもも\t名詞,一般,*,*,*,*,すもも,スモモ,スモモ\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "もも\t名詞,一般,*,*,*,*,もも,モモ,モモ\n",
      "も\t助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "もも\t名詞,一般,*,*,*,*,もも,モモ,モモ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "うち\t名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ\n"
     ]
    }
   ],
   "source": [
    "for tok in toks:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ご飯\t名詞,一般,*,*,*,*,ご飯,ゴハン,ゴハン\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "食べ\t動詞,自立,*,*,一段,連用形,食べる,タベ,タベ\n",
      "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n"
     ]
    }
   ],
   "source": [
    "toks = t.tokenize('ご飯を食べた')\n",
    "for tok in toks:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Janome の解析結果確認\n",
    "\n",
    "\n",
    "`tokenize()` は形態素解析の結果をリストとして返しますが、リストの要素それぞれに `surface` （表層形）、 `part_of_speech`（品詞）、 `infl_type` （活用型1）、`infl_form`（活用形2）、 `base_form`（基本形、見出し語）、 `reading`（読み）、 `phonetic`（発音）という要素があります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "表層形：西郷\n",
      "品詞情報：名詞,固有名詞,人名,姓\n",
      "活用形１：*\n",
      "活用形２：*\n",
      "基本形：西郷\n",
      "読み：サイゴウ\n",
      "---------------------\n",
      "表層形：隆盛\n",
      "品詞情報：名詞,固有名詞,人名,名\n",
      "活用形１：*\n",
      "活用形２：*\n",
      "基本形：隆盛\n",
      "読み：タカモリ\n",
      "---------------------\n",
      "表層形：は\n",
      "品詞情報：助詞,係助詞,*,*\n",
      "活用形１：*\n",
      "活用形２：*\n",
      "基本形：は\n",
      "読み：ハ\n",
      "---------------------\n",
      "表層形：ご飯\n",
      "品詞情報：名詞,一般,*,*\n",
      "活用形１：*\n",
      "活用形２：*\n",
      "基本形：ご飯\n",
      "読み：ゴハン\n",
      "---------------------\n",
      "表層形：を\n",
      "品詞情報：助詞,格助詞,一般,*\n",
      "活用形１：*\n",
      "活用形２：*\n",
      "基本形：を\n",
      "読み：ヲ\n",
      "---------------------\n",
      "表層形：食べ\n",
      "品詞情報：動詞,自立,*,*\n",
      "活用形１：一段\n",
      "活用形２：連用形\n",
      "基本形：食べる\n",
      "読み：タベ\n",
      "---------------------\n",
      "表層形：た\n",
      "品詞情報：助動詞,*,*,*\n",
      "活用形１：特殊・タ\n",
      "活用形２：基本形\n",
      "基本形：た\n",
      "読み：タ\n",
      "---------------------\n",
      "表層形：。\n",
      "品詞情報：記号,句点,*,*\n",
      "活用形１：*\n",
      "活用形２：*\n",
      "基本形：。\n",
      "読み：。\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "toks = t.tokenize('西郷隆盛はご飯を食べた。')\n",
    "for tok in toks:\n",
    "    print('表層形：' + tok.surface)\n",
    "    print('品詞情報：'+ tok.part_of_speech)\n",
    "    print('活用形１：'+ tok.infl_type)\n",
    "    print('活用形２：'+ tok.infl_form)\n",
    "    print('基本形：'+ tok.base_form)\n",
    "    print('読み：'+ tok.reading)\n",
    "    print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ファイルからの読み込み\n",
    "さて、上では `tokenzie()` に日本語の文章を直接指定しましたが、一般には、別に用意されたファイルから文章を読み込むのが普通です。まずファイルを用意します。\n",
    "次のように書かれたファイル short.txt を data フォルダに用意します。\n",
    "\n",
    "> 国境の長いトンネルを抜けると雪国であった。\n",
    "\n",
    "Windows ではメモ帳を使うことができますが、保存の際、文字コードとして `UTF-8' を選んでください。このファイルを、以下で表示されるフォルダに保存します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/2bddf92b-47f9-4809-95a5-b91e7f25af27/myData/GitHub/python_de_textmining'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "国境\n",
      "の\n",
      "長い\n",
      "トンネル\n",
      "を\n",
      "抜ける\n",
      "と\n",
      "雪国\n",
      "で\n",
      "あっ\n",
      "た\n",
      "。\n"
     ]
    }
   ],
   "source": [
    "## テキストファイルの読み取り\n",
    "f = open('data/short.txt', 'r', encoding='utf-8')\n",
    "text = f.read()\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "for token in t.tokenize(text):\n",
    "    print(token.surface)\n",
    "\n",
    "## テキストファイルを閉じる\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "さて、少し複雑な処理を試してみましょう。テキスト分析では、形態素を大きく内容語と機能語に分けて考えます。内容語は、テキストのテーマを端的に表す形態素、機能語は文法的に重要な形態素です。助詞の「は」や「が」などが代表ですが、出現頻度から書き手の推定に有効だと考えられています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['国境', 'トンネル', '雪国']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('data/short.txt', 'r', encoding='utf-8')\n",
    "text = f.read()\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "## janomeの形態素解析から抜き出す名詞リスト\n",
    "meishi_lst = []\n",
    "\n",
    "for token in t.tokenize(text):\n",
    "    ## 品詞が名詞なら\n",
    "    if '名詞' in token.part_of_speech:\n",
    "        ## 表層形(語句)を出力\n",
    "        meishi = token.surface\n",
    "        ## 名詞リストに追加\n",
    "        meishi_lst.append(meishi)\n",
    "\n",
    "## テキストファイルを閉じる\n",
    "f.close()\n",
    "\n",
    "meishi_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 青空文庫\n",
    "\n",
    "さて、ここで本格的なテキスト処理に挑戦してみましょう。青空文庫からテキストを取り出して、形態素解析にかけるという処理を行ってみます。その上で、固有名詞を取り出してみます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download URL\n",
      "URL: https://www.aozora.gr.jp/cards/000035/files/1567_ruby_4948.zip\n",
      "1567_ruby_4948/hashire_merosu.txt\n",
      "ファイルの作成：hashire_merosu.txt\n"
     ]
    }
   ],
   "source": [
    "from AozoraDL import aozora\n",
    "## URL を文字列として指定\n",
    "aozora('https://www.aozora.gr.jp/cards/000035/files/1567_ruby_4948.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['王', '王', 'セリヌンティウス', '王', 'えい', 'えい', '二里', '三里', '猛', '木葉', '渡', 'ゼウス', '韋駄天', '潺々', '清水', 'ゼウス', '小川', 'はるか', 'メロス', '勇', 'ちか', 'メロス', 'メロス']\n"
     ]
    }
   ],
   "source": [
    "f = open('hashire_merosu.txt', 'r')\n",
    "text = f.read()\n",
    "t = Tokenizer()\n",
    "\n",
    "## janomeの形態素解析から抜き出す名詞リスト\n",
    "meishi_lst = []\n",
    "\n",
    "for token in t.tokenize(text):\n",
    "    ## 品詞細分類が固有名詞なら\n",
    "    if \"固有名詞\" in token.part_of_speech:\n",
    "        ## 表層形(語句)を出力\n",
    "        meishi = token.surface\n",
    "        ## 名詞リストに追加\n",
    "        meishi_lst.append(meishi)\n",
    "\n",
    "## テキストファイルを閉じる\n",
    "f.close()\n",
    "## リストを表示\n",
    "print(meishi_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 自作関数のモジュール化\n",
    "\n",
    "さて、形態素解析の結果を使って分析を行う場合、利用する品詞情報を指定したくなります。具体的には、名詞や動詞、形容詞のみを抽出してデータとしたいことがあります。先にみたように、形態素ごとの品詞情報は、形態素ごとに `part_of_speech` として保存されています。そこで、これが名詞か動詞、あるいは形容詞だった場合だけ形態素をデータとして取り出すことを考えます。本書ではこの処理を繰り返し行いますので、これを関数として定義してしまいましょう。また、関数定義を別ファイルとして保存して、これをモジュールとして読み込む方法を紹介します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "def tokens(text, pos = ['名詞','形容詞','動詞']):\n",
    "    word_list = []\n",
    "    for token in t.tokenize(text):\n",
    "    ## 品詞が名詞なら\n",
    "        tp = (token.part_of_speech).split(',')\n",
    "        if tp[0] in pos:\n",
    "            word_list.append(token.base_form)\n",
    "    return word_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    out = tokens('これは良い本です。')\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "ちなみにターミナルを起動して、以下のように実行することもできます。\n",
    "\n",
    "```\n",
    "$ python my_janome.py \n",
    "['これ', '良い', '本']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "デフォルトでは名詞、形容詞、助詞を出力\n",
      "['ランチ', '食べる']\n",
      "助詞のみ抽出\n",
      "['を']\n"
     ]
    }
   ],
   "source": [
    "import my_janome\n",
    "print('デフォルトでは名詞、形容詞、助詞を出力')\n",
    "out = my_janome.tokens('ランチを食べました。')\n",
    "print(out)\n",
    "print('助詞のみ抽出')\n",
    "out = my_janome.tokens(\"ランチを食べました。\", pos = '助詞')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "## my_janome.py \n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "def tokens(text, pos = ['名詞','形容詞','動詞']):\n",
    "    word_list = []\n",
    "    for token in t.tokenize(text):\n",
    "    ## 品詞が名詞なら\n",
    "        tp = (token.part_of_speech).split(',')\n",
    "        if len(pos) < 1 or tp[0] in pos:\n",
    "            word_list.append(token.base_form)\n",
    "    return word_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    out = tokens('これは良い本です。')\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ストップワード\n",
    "\n",
    "\n",
    "たとえば、デフォルトの Janome で「今日は本を読んで過ごした。」を形態素解析してみると、以下のような結果になります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今日\t名詞,副詞可能,*,*,*,*,今日,キョウ,キョー\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "この\t連体詞,*,*,*,*,*,この,コノ,コノ\n",
      "本\t名詞,一般,*,*,*,*,本,ホン,ホン\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "読み\t動詞,自立,*,*,五段・マ行,連用形,読む,ヨミ,ヨミ\n",
      "ます\t助動詞,*,*,*,特殊・マス,基本形,ます,マス,マス\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n"
     ]
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer()\n",
    "toks = t.tokenize('今日はこの本を読みます。')\n",
    "for tok in toks:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Janome の解析結果では、`part_of_speech` を参照すれば、必要な品詞情報だけを取り出すことができるようになります。作成したばかりのモジュールを使って、次の文章を解析してみましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['これ', '良い', '本']\n"
     ]
    }
   ],
   "source": [
    "import my_janome\n",
    "out = my_janome.tokens('これは良い本だ。')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「これ」が名詞として抽出されていますが、内容語とはいえないでしょう。ちなみに、「これ」の品詞細分類を確認してみましょう。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "名詞,代名詞,一般,*\n"
     ]
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer()\n",
    "tokens = t.tokenize('これは良い本だ。')\n",
    "for tok in tokens:\n",
    "    if tok.surface == 'これ':\n",
    "        print(tok.part_of_speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "機能語と判断される形態素をあらかじめリストにしておいて、これと照合することで不要語を一気に削除してしまう方法もあります。こうしたリストをストップワードと言います。分析目的にあわせてストップワードは自身を作成するのがベストですが、公開されているリストを利用することもできます。こうしたリストとして、京都大学情報学研究科社会情報学専攻田中克己研究室が公開している SlothLib [^SlothLib] がありますので、ここで利用させてもらいましょう。\n",
    "\n",
    "[^SlothLib]:  http://www.dl.kuis.kyoto-u.ac.jp/slothlib/\n",
    "\n",
    "\n",
    "ここでは stopword リストを stopwords.txt として、data フォルダに保存します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "urllib.request.urlretrieve(url, 'data/stopwords.txt')\n",
    "stopwords = []\n",
    "with open('data/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "\tstopwords = [w.strip() for w in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['良い', '本']\n"
     ]
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer()\n",
    "tokens = t.tokenize('これは良い本です。')\n",
    "pos = ['名詞','形容詞','動詞']\n",
    "word_list = []\n",
    "for token in tokens:\n",
    "    tp = (token.part_of_speech).split(',')\n",
    "    if token.base_form not in stopwords:\n",
    "        if tp[0] in pos :\n",
    "            word_list.append(token.base_form)\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "##  my_janome_stopwords.py\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "def tokens(text, pos = ['名詞','形容詞','動詞'] , stopwords_list=[]):\n",
    "    word_list = []\n",
    "    for token in t.tokenize(text):\n",
    "    ## 品詞が名詞なら\n",
    "        tp = (token.part_of_speech).split(',')\n",
    "        if token.base_form not in stopwords_list:\n",
    "            if len(pos) < 1 or (tp[0] in pos):\n",
    "                word_list.append(token.base_form)\n",
    "    return word_list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    out = tokens('これは良い本です。')\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['これ', '良い', '本']\n"
     ]
    }
   ],
   "source": [
    "import my_janome_stopwords as jnm\n",
    "out = jnm.tokens('これは良い本です。')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['良い', '本']\n"
     ]
    }
   ],
   "source": [
    "import my_janome_stopwords as jnm\n",
    "out = jnm.tokens('これは良い本です。', stopwords_list=stopwords)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本書の分析例では形態素解析器として MeCab を利用したコードを掲載しますが、MeCab をインポートする行を `import my_janome_stopwords as jnm` と変えることで、同様の出力を得ることができます（多少、異なる結果になることもあります）。\n",
    "ただし、MeCab に比べると Janome の解析速度は非常に遅いので、入力データ（日本語文章）が大きくなると、結果が得られるまでかなり時間がかかることがあります。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "name": "Chapter05_intro_janome.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
